{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 提升训练速度的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.对连接的权重采用一个更好的初始化策略  \n",
    "2.使用一个更好的激活函数  \n",
    "3.在训练时使用Batch Normalization  \n",
    "4.复用一个已经预先训练过的神经网络  \n",
    "5.使用一个相比梯度下降更快的优化器:  \n",
    "e.g.,Momentum optimizaion, Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "目前最快的优化器是: Adam optimization  \n",
    "Adam optimization通常是梯度下降速度的数倍  \n",
    "Adam optimization没有一些可调的超参数(包括学习率)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "Adam optimization的使用:  \n",
    "只需要简单地将GradientDescentOptimizer替换为AdamOptimizer  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过正则化避免过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在验证集上的表现开始下降时停止训练  \n",
    "在TensorFlow上的实现为:  \n",
    "1.在固定的间隔(e.g.,50 steps)评估模型在验证集上的表现    \n",
    "2.保存当前最优表现的快照  \n",
    "3.在达到限定的步数时(e.g.,2000 steps),停止训练\n",
    "4.计算最后的一个最优表现需要训练的步数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. $l1,l2$正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001   #设置kernel_regularizer参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))   # 将activation,kernel_regularizer参数代入tf.layers.dense\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # 定义损失命名空间\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # 定义交叉熵\n",
    "        labels=y, logits=logits)                                \n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # 定义基础损失函数\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)  # 定义正则化项\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")       # tf.add_n([p1, p2, p3....])函数是实现一个列表的元素的相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最流行的深度学习正则化技术无疑是dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法步骤:  \n",
    "1.在每个训练step, 每个神经元都有一定的概率p被临时剔除 (包括输出层,但不包括输出层)  \n",
    "2.在下一个step中, 这些神经元将被重新激活  \n",
    "3.训练后,神经元将不再会被剔除\n",
    "超参数p称为dropout rate, 通常设置为50%  \n",
    "经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个重要的技术细节:  \n",
    "假定P=50%, 那么在训练时,只有差不多一半的输入神经会出现  \n",
    "因此,在测试和预测时,一个神经元将连接2倍于训练时输入神经的数量  \n",
    "为了补偿这一情况, 需要在训练后, 对每一个输入神经元的连接权重乘以0.5  \n",
    "一般地,在训练后,需要对每一个输入连接权重乘以一个 keep probability: $(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要在输入层和每一层隐含层中应用tf.layers.dropout函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)  # 使用的函数为tf.layers.dropout\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果发现模型过拟合,应该增加dropout rate (即减少keep_prob)  \n",
    "相反,如果发现模型欠拟合,则减小dropout rate  \n",
    "同时,对于较大的层增加dropout rate; 对于较小的层减少dropout rate也很有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout会显著地降低收敛的时间,但只要调节得当,通常会带来更好的模型  \n",
    "对于额外的训练时间,使用该方法绝对是物有所值的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一个神经元,限制其连接的权重 $\\left\\| \\mathbf{w} \\right\\|_2<=r$  \n",
    "其中$r$为max-norm的超参数, 而 $\\left\\| \\mathbf{·} \\right\\|_2$ 为 $l_2$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.在每一step后,计算$\\left\\| \\mathbf{w} \\right\\|_2$  \n",
    "2.在需要的情况下,降低 $\\mathbf{w}$ $ \\left( \\mathbf{w} \\gets \\mathbf{w} \\dfrac{r}{\\left\\| \\mathbf{w} \\right\\|_2} \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减少$r$值可以增加正则化效果,并消减过度拟合  \n",
    "如果没有使用Batch Normalization, max-norm也能缓解vanishing/exploding gradients问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.,如果想要分类图片,可以对原始图片进行偏移,旋转和选择尺寸  \n",
    "这种方法强迫模型对图片的位置,方向和尺寸有更大的容忍度  \n",
    "同时,经过该方法,可以极大地增加训练集的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践指导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "初始化:  He initialization  \n",
    "激活函数:  ELU  \n",
    "标准化:  Batch Normalization  \n",
    "正则化:  Dropout  \n",
    "优化器(Optimizer):  Adam  \n",
    "学习率schedule:  None\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他一些可作调整的默认设置:  \n",
    "1.如果收敛过慢,则增加学习率; 如果收敛速度较快,但准确率不理性,则可以使用指数衰减的学习率schedule  \n",
    "2.如果训练数据过少,则可以使用数据增强(Data Augmentation)  \n",
    "3.如果想要得到一个稀疏的模型,可以使用$l_1$正则化,如果需要更稀疏的模型,可以将Adam optimization替换成FTRL(同时使用$l_1$)  \n",
    "4.如果需要模型在运行期间有光速般的效率,可以去除Batch Normalization, 将ELU激活函数替换为leaky ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
